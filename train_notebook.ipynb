{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning import Trainer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from undecorated import undecorated\n",
    "from types import MethodType\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-US-data_dir=MASSIVE\n",
      "Reusing dataset massive (C:\\Users\\andre\\.cache\\huggingface\\datasets\\qanastek___massive\\en-US-data_dir=MASSIVE\\1.0.0\\31cdffab94ac97bfe5a394b1e96344c96f0ad847e1d796c7562d8c8b449e22e6)\n",
      "100%|██████████| 3/3 [00:00<00:00, 221.13it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"qanastek/MASSIVE\", \"en-US\", data_dir='MASSIVE')\n",
    "\n",
    "_INTENTS = ['audio_volume_other', 'play_music', 'iot_hue_lighton', 'general_greet', 'calendar_set', 'audio_volume_down', 'social_query', 'audio_volume_mute', 'iot_wemo_on', 'iot_hue_lightup', 'audio_volume_up', 'iot_coffee', 'takeaway_query', 'qa_maths', 'play_game', 'cooking_query', 'iot_hue_lightdim', 'iot_wemo_off', 'music_settings', 'weather_query', 'news_query', 'alarm_remove', 'social_post', 'recommendation_events', 'transport_taxi', 'takeaway_order', 'music_query', 'calendar_query', 'lists_query', 'qa_currency', 'recommendation_movies',\n",
    "            'general_joke', 'recommendation_locations', 'email_querycontact', 'lists_remove', 'play_audiobook', 'email_addcontact', 'lists_createoradd', 'play_radio', 'qa_stock', 'alarm_query', 'email_sendemail', 'general_quirky', 'music_likeness', 'cooking_recipe', 'email_query', 'datetime_query', 'transport_traffic', 'play_podcasts', 'iot_hue_lightchange', 'calendar_remove', 'transport_query', 'transport_ticket', 'qa_factoid', 'iot_cleaning', 'alarm_set', 'datetime_convert', 'iot_hue_lightoff', 'qa_definition', 'music_dislikeness']\n",
    "_TAGS = ['O', 'B-food_type', 'B-movie_type', 'B-person', 'B-change_amount', 'I-relation', 'I-game_name', 'B-date', 'B-movie_name', 'I-person', 'I-place_name', 'I-podcast_descriptor', 'I-audiobook_name', 'B-email_folder', 'B-coffee_type', 'B-app_name', 'I-time', 'I-coffee_type', 'B-transport_agency', 'B-podcast_descriptor', 'I-playlist_name', 'B-media_type', 'B-song_name', 'I-music_descriptor', 'I-song_name', 'B-event_name', 'I-timeofday', 'B-alarm_type', 'B-cooking_type', 'I-business_name', 'I-color_type', 'B-podcast_name', 'I-personal_info', 'B-weather_descriptor', 'I-list_name', 'B-transport_descriptor', 'I-game_type', 'I-date', 'B-place_name', 'B-color_type', 'B-game_name', 'I-artist_name', 'I-drink_type', 'B-business_name', 'B-timeofday', 'B-sport_type', 'I-player_setting', 'I-transport_agency', 'B-game_type', 'B-player_setting', 'I-music_album', 'I-event_name', 'I-general_frequency', 'I-podcast_name', 'I-cooking_type', 'I-radio_name', 'I-joke_type',\n",
    "         'I-meal_type', 'I-transport_type', 'B-joke_type', 'B-time', 'B-order_type', 'B-business_type', 'B-general_frequency', 'I-food_type', 'I-time_zone', 'B-currency_name', 'B-time_zone', 'B-ingredient', 'B-house_place', 'B-audiobook_name', 'I-ingredient', 'I-media_type', 'I-news_topic', 'B-music_genre', 'I-definition_word', 'B-list_name', 'B-playlist_name', 'B-email_address', 'I-currency_name', 'I-movie_name', 'I-device_type', 'I-weather_descriptor', 'B-audiobook_author', 'I-audiobook_author', 'I-app_name', 'I-order_type', 'I-transport_name', 'B-radio_name', 'I-business_type', 'B-definition_word', 'B-artist_name', 'I-movie_type', 'B-transport_name', 'I-email_folder', 'B-music_album', 'I-house_place', 'I-music_genre', 'B-drink_type', 'I-alarm_type', 'B-music_descriptor', 'B-news_topic', 'B-meal_type', 'I-transport_descriptor', 'I-email_address', 'I-change_amount', 'B-device_type', 'B-transport_type', 'B-relation', 'I-sport_type', 'B-personal_info']\n",
    "\n",
    "\n",
    "def index_to_intent(index):\n",
    "    return _INTENTS[index]\n",
    "\n",
    "\n",
    "def index_to_tag(index):\n",
    "    return _TAGS[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent_slots():\n",
    "    intent_slots = {}\n",
    "    for data in dataset['train']:\n",
    "        intent = data['intent']\n",
    "        if intent not in intent_slots:\n",
    "            intent_slots[intent] = set(data['ner_tags'])\n",
    "        else:\n",
    "            intent_slots[intent] = intent_slots[intent].union(\n",
    "                set(data['ner_tags']))\n",
    "\n",
    "    named_data = {}\n",
    "    for intent, slots in intent_slots.items():\n",
    "        named_data[intent] = 'slots: ' + \", \".join(set(\n",
    "            [index_to_tag(slot)[2:] if index_to_tag(slot) != 'O' else 'O' for slot in slots]))\n",
    "    return named_data\n",
    "\n",
    "\n",
    "def get_data_per_intent():\n",
    "    data_per_intent = {}\n",
    "    for data in dataset['train']:\n",
    "        intent = data['intent']\n",
    "        if intent not in data_per_intent:\n",
    "            data_per_intent[intent] = [data]\n",
    "        else:\n",
    "            data_per_intent[intent].append(data)\n",
    "    return data_per_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5GenerationFineTune(Dataset):\n",
    "    def __init__(self, dataset, intent_slots, tokenizer, data_per_intent, max_length=512):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.intent_slots = intent_slots\n",
    "        self.data_per_intent = data_per_intent\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    # format of data (intent, slots_for_intent, example: text), all_examples_except_current\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        intent = data['intent']\n",
    "        slots_for_intent = self.intent_slots[intent]\n",
    "        text = data['text']\n",
    "        all_examples_except_current = self.data_per_intent[intent][:index] + \\\n",
    "            self.data_per_intent[intent][index+1:]\n",
    "        all_examples_except_current = [example['text']\n",
    "                                       for example in all_examples_except_current]\n",
    "\n",
    "        input_text = f\"intent: {index_to_intent(intent)}\\nslots: {slots_for_intent}\\nexample: {text}\"\n",
    "        tokenized_text = self.tokenizer(\n",
    "            input_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        tokenized_examples = [self.tokenizer(example, max_length=self.max_length, padding='max_length',\n",
    "                                             truncation=True, return_tensors='pt') for example in all_examples_except_current]\n",
    "\n",
    "        return tokenized_text.input_ids, tokenized_text.attention_mask, torch.Tensor(tokenized_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5GenerationDataModule(LightningDataModule):\n",
    "    def __init__(self, dataset, tokenizer, batch_size=8, max_length=512):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.intent_slots = get_intent_slots()\n",
    "        self.data_per_intent = get_data_per_intent()\n",
    "        self.train_dataset = T5GenerationFineTune(\n",
    "            self.dataset['train'], self.intent_slots, self.tokenizer, self.data_per_intent, self.max_length)\n",
    "        self.val_dataset = T5GenerationFineTune(\n",
    "            self.dataset['validation'], self.intent_slots, self.tokenizer, self.data_per_intent, self.max_length)\n",
    "        self.test_dataset = T5GenerationFineTune(\n",
    "            self.dataset['test'], self.intent_slots, self.tokenizer, self.data_per_intent, self.max_length)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5DataGenerator(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            self.args['model_name'])\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(\n",
    "            self.args['tokenizer_name'])\n",
    "\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # allow grad in generate\n",
    "        generate_with_grad = undecorated(self.model.generate)\n",
    "        self.model.generate_with_grad = MethodType(\n",
    "            generate_with_grad, self.model)\n",
    "\n",
    "    # have the model generate multiple unique candidate outputs for a given input\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model.generate_with_grad(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            do_sample=True,\n",
    "            max_length=50,\n",
    "            top_k=self.args['top_k'],\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=self.args['num_return_sequences'],\n",
    "        )\n",
    "\n",
    "    # for a given training step, generate a batch of candidate outputs for a given input\n",
    "    # input_ids are the tokenized examples of the data we want to generate\n",
    "    # the labels are all of the potential outputs the model could generate for a given intent\n",
    "    # model out shape (bs * num_return_sequences, max_seq_len)\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, all_but_example = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # for each output sequence, calculate the least loss relative to all of teh examples in labels\n",
    "        outputs = outputs.view(\n",
    "            len(input_ids), self.args['num_return_sequences'], -1)\n",
    "\n",
    "        loss = torch.sum([torch.min([self.loss(gen, data)\n",
    "                         for xs in outputs for gen in xs]) for data in all_but_example])\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, all_but_example = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # for each output sequence, calculate the least loss relative to all of teh examples in labels\n",
    "        outputs = outputs.view(\n",
    "            len(input_ids), self.args['num_return_sequences'], -1)\n",
    "\n",
    "        print(outputs.shape)\n",
    "        print('calculating loss')\n",
    "        loss = torch.sum([torch.min([self.loss(gen, data)\n",
    "                         for xs in outputs for gen in xs]) for data in all_but_example])\n",
    "        print('loss calculated')\n",
    "        self.log('validation_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'model_name': 't5-small',\n",
    "    'tokenizer_name': 't5-small',\n",
    "    'top_k': 5,\n",
    "    'num_return_sequences': 5,\n",
    "}\n",
    "\n",
    "model = T5DataGenerator(args)\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "data_module = T5GenerationDataModule(dataset, tokenizer, batch_size=8)\n",
    "\n",
    "logger = WandbLogger(project='t5-data-generator', name='sanity_check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: c:\\Users\\andre\\Documents\\Coding Things\\NLU data gen\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "1 | loss  | CrossEntropyLoss           | 0     \n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)\n",
    "trainer.fit(model, data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a4fd3c0dcbc658b189fbf5814096262f6268a6dc22fca0ed725de03044a0d6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
